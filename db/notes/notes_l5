Slotted page, log-structured, index-organized are ideal for write-heavy workloads (INSERT/UPDATE/DELETE)

Database Workloads
    OLTP
        Fast operations that only read/update a small amount of data each time
    OLAP
        Complex queries that read a lot of data to compute aggregates
    Hyrbid
        OLTP + OLAP together on a single db instance

The relational model does not specify anything about how we should store data on disk
    ie, all tuple attributes together

Storage Models
    DBMS storage model specifies how it physically organizes tuples on disk and in memory
        Can have different performance characteristics based on target workload
        Influences the design choices of the rest of the DBMS

    Common choices:
        N-ary Storage Model (NSM)
            (Row store)
            DBMS stores almost all attributes for a single tuple contiguously in a single page
            Ideal for OLTP workload where queries are more likely to access individual entities and do writes
            NSM page sizes are typically some constant multiple of 4KB hw pages
            Tuples record ID (page#, slot#) is the DMBS unique identifier

        Decomposition Storage Model (DSM)
            (Column store)
            DBMS stores a single attribute for all tuples contiguously in a block of data
            Ideal for OLAP workloads where read-only queries perform large scans over a subset of the attributes
            DBMS is responsible for combining/splitting a tuple's attribute when reading/writing
            Store attributes and metadata in separate array of FIXED-LENGTH values
                Most systems identify unique physical tuples using offsets into these arrays
                Need to handle variable-length values
                    Max length? Pointers to overflow pages?
            Tuple Identification
                Fixed-length offsets
                    Each value is the same length for an attribute
                    This is the way
                    How to handle variable-length attributes?
                        Dictionary compression
                        Convert variable-length data into u32
                Embedded tuple IDs
                    Each value is stored with its tuple id in a column
            
            Column store history:
                1970s: Cantor
                1980s: DSM Proposal paper
                1990s: SybaseIQ (in-memory only)
                2000s: Vertica, Vectorwise, MonetDB
                2010s: Parquet, Snowflake, everyone

Observation:
    OLAP queries almost never access a single column by itself
    At some point during execution, the DBMS must get other columns and stitch the original tuple back together

        Hybrid Storage Model (PAX - Partition Attribute Across)
            (Column store*)
            Hybrid storage model that vertically partitions attributes within a db page
                This is what Parquet does
            Horizontally partition rows into groups
            Vertically partition attributes into columns
            Global header contains directory with offsets to row groups
            Each row group has its own metadata header about its contents

Observation:
    I/O is the main bottleneck if the dbms fetches data from disk during query execution
    DBMS can compress pages to increase the utility of the data moved per I/O operation
    Key trade-off is speed vs compression ratio
        Compressing the db reduces DRAM requirements
        It may decrease CPU costs during query execution


Database compression:
    Goal 1:
        Must produce fixed-length values
            Exeception for variable-length data stored in separate pool
    Goal 2:
        Postpone decompression for as long as possible during query execution
            "late materialization"
    Goal 3:
        Must be lossless

    Compression granularity:
        Choice 1:
            Compress a block of tuples for the same table
                Compress data using a general-purpose algorithm
                Scope of compression is only based on the data provided as input
                    LZO, LZ4, Snappy, OZIP, Zstd
                The DMBS must decompress data first before it can be read and (potentially) modified
Observation:
    Ideally, we want the DBMS to operate on compressed data without decompressing it first

        Choice 2:
            Compress the contents of the entire tuple (NSM)

        Choice 3:
            Compress a single attribute within one tuple (overflow)
            Can target multiple attributes for the same tuple

        Choice 4:
            Compress multiple values for one or more attributes stored for multiple tuples (DSM)
                RLE
                    Compress runs of the same value in a single column into triplets
                        Value
                        Start position
                        # of elements
                    Maximum compression when data is sorted

                Bit-Packing Encoding
                    If values for an integer attribute is smaller than the range of the data type size, reduce the number of bits to represent each value
                    Use bit-shifting black magic to operate on multiple values in a single word
                    Mostly encoding:
                        A variation of bit packing for when a columns values are "mostly" less than a size
                        Remaining values that cannot be compressed are stored in raw form elsewhere

                Bitmap Encoding
                    Store a separate bitmap for each unique value for an attribute where an offset in the vector corresponds to a tuple
                        The i-th position in the bitmap corresponds to the i-th tuple in the table
                        Typically segmented into chunks to avoid allocating large blocks of contiguous memory
                        Practical if cardinality is low

                Delta Encoding
                    Recording the difference between values that follow each other in the same column
                        Store base value in-line or in a separate look-up table
                        Combine with RLE for even better compression

                Dictionary Encoding
                    Replace frequent values with a small, fixed-length codes and then maintain a dictionary to get the original values
                        Typically, one code per attribute value
                        Most widely used native compression scheme in DMBS
                    Ideal dictionary scheme supports fast encoding and decoding for both point and range queries
                    Dictionary must support two ops:
                        Encode/Locate
                        Decode/Extract
                    No magic hash function can do this for use
                        Must maintain our own structure for this
                    CAN MAKE `DISTINCT` IMPROVE QUERY PERFORMANCE
                    
                    Dictionary Data Structures:
                        Choice 1:
                            Array
                                One array of variable-length strings and another array with pointers that maps to string offsets
                                Expensive to update so only usable in immutable files
                        Choice 2:
                            Hash Table
                                Fast and compact
                                Unable to support range and prefix queries
                        Choice 3:
                            B+Tree
                                Slower than hash table and takes more memory
                                Can support range and prefix queries

